# import numpy as np
# import streamlit as st
# from openai import OpenAI
# from utils.data_loader import load_index

# # ---- Config OpenAI ----
# client = OpenAI(api_key=st.secrets["KEY"]["OPENAI_API_KEY"])

# # ---- Charger index vectoriel ----
# if "df_index" not in st.session_state:
#     st.session_state["df_index"] = load_index()

# df_index = st.session_state["df_index"]

# # ---- Liste des √©tablissements disponibles ----
# ETABS = sorted(df_index["doc"].unique())

# # ---- Similarit√© cosinus ----
# def cosine_similarity(a, b):
#     return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

# # ---- D√©tection d‚Äô√©tablissement dans la question ----
# def detect_etab(query: str) -> str | None:
#     for etab in ETABS:
#         # d√©tection simple (nom du doc ou partie du nom dans la question)
#         if etab.lower().replace("_", " ") in query.lower():
#             return etab
#     return None

# # ---- Recherche ----
# def search(query, etab=None, top_k=3, model="text-embedding-3-small"):
#     resp = client.embeddings.create(model=model, input=query)
#     query_emb = np.array(resp.data[0].embedding)

#     sub_df = df_index if etab is None else df_index[df_index["doc"] == etab]

#     sims = []
#     for _, row in sub_df.iterrows():
#         if row["embedding"] is None:
#             continue
#         emb = np.array(row["embedding"])
#         sims.append((cosine_similarity(query_emb, emb), row["doc"], row["page"], row["text"]))

#     return sorted(sims, key=lambda x: x[0], reverse=True)[:top_k]

# # ---- G√©n√©ration r√©ponse ----
# def answer_question(query, top_k=3):
#     etab = detect_etab(query)
#     results = search(query, etab=etab, top_k=top_k)

#     context = "\n\n".join([f"Doc: {doc} (p.{page})\n{text}" for _, doc, page, text in results])

#     if etab:
#         system_prompt = f"Tu es un assistant sp√©cialis√©. R√©ponds UNIQUEMENT pour l‚Äô√©tablissement : {etab}. Ignore tout extrait d‚Äôautres √©tablissements."
#     else:
#         system_prompt = "Tu es un assistant sp√©cialis√©. R√©ponds en t‚Äôappuyant uniquement sur les extraits ci-dessous."

#     resp = client.chat.completions.create(
#         model="gpt-5",
#         messages=[
#             {"role": "system", "content": system_prompt},
#             {"role": "user", "content": f"Question: {query}\n\nExtraits:\n{context}"}
#         ]
#     )
#     return resp.choices[0].message.content

# # ---- UI ----
# st.header("üí¨ Smart Chat")

# if "messages" not in st.session_state:
#     st.session_state["messages"] = []

# # Afficher l‚Äôhistorique
# for msg in st.session_state["messages"]:
#     with st.chat_message(msg["role"]):
#         st.markdown(msg["content"])

# # Input utilisateur
# if query := st.chat_input("Pose ta question sur les rapports d‚Äôhomologation..."):
#     st.session_state["messages"].append({"role": "user", "content": query})
#     with st.chat_message("user"):
#         st.markdown(query)

#     answer = answer_question(query)
#     st.session_state["messages"].append({"role": "assistant", "content": answer})
#     with st.chat_message("assistant"):
#         st.markdown(answer)


import numpy as np
import streamlit as st
from openai import OpenAI
from utils.data_loader import load_index

# ---- Config OpenAI ----
client = OpenAI(api_key=st.secrets["KEY"]["OPENAI_API_KEY"])

# ---- Charger index vectoriel ----
if "df_index" not in st.session_state:
    st.session_state["df_index"] = load_index()

df_index = st.session_state["df_index"]

# ---- Liste des √©tablissements disponibles ----
ETABS = sorted(df_index["doc"].unique())

# ---- Similarit√© cosinus ----
def cosine_similarity(a, b):
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

# ---- D√©tection d‚Äô√©tablissement dans la question ----
def detect_etab(query: str) -> str | None:
    for etab in ETABS:
        if etab.lower().replace("_", " ") in query.lower():
            return etab
    return None

# ---- Recherche ----
def search(query, etab=None, top_k=5, model="text-embedding-3-small"):
    resp = client.embeddings.create(model=model, input=query)
    query_emb = np.array(resp.data[0].embedding)

    sub_df = df_index if etab is None else df_index[df_index["doc"] == etab]

    sims = []
    for _, row in sub_df.iterrows():
        if row["embedding"] is None:
            continue
        emb = np.array(row["embedding"])
        sims.append((cosine_similarity(query_emb, emb), row["doc"], row["page"], row["text"]))

    return sorted(sims, key=lambda x: x[0], reverse=True)[:top_k]

# ---- G√©n√©ration r√©ponse (non streaming) ----
def answer_question(query, top_k=5):
    etab = detect_etab(query)
    results = search(query, etab=etab, top_k=top_k)

    context = "\n\n".join([f"[{doc}, p.{page}] {text[:800]}..." for _, doc, page, text in results])

    if etab:
        system_prompt = f"""
        Tu es un assistant expert en analyse et synth√®se de rapports d‚Äôhomologation et de suivi
        d‚Äô√©tablissements fran√ßais √† l‚Äô√©tranger.

        Contexte : tu aides l‚Äô√©quipe de pilotage de la Mission La√Øque Fran√ßaise / OSUI (t√™te de r√©seau)
        √† exploiter ces documents. R√©ponds UNIQUEMENT pour l‚Äô√©tablissement : {etab}.

        R√®gles de r√©ponse :
        - Appuie-toi uniquement sur les extraits fournis (ne jamais inventer).
        - Structure syst√©matiquement la r√©ponse avec des titres clairs.
        - Mets les √©l√©ments en liste √† puces pour la lisibilit√©.
        - Int√®gre toujours les r√©f√©rences de pages quand elles sont disponibles.
        - Organise la r√©ponse autour des rubriques suivantes si possible :
            * Gouvernance et contexte
            * Atouts et points forts
            * Points de vigilance / critiques
            * Recommandations et axes de travail prioritaires
            * Enjeux pour le pilotage r√©seau (MLF/OSUI)
        - Si la question implique une comparaison avec d‚Äôautres √©tablissements, pr√©sente une
        analyse comparative structur√©e.
        - Termine par une courte synth√®se strat√©gique orient√©e ‚Äút√™te de r√©seau‚Äù.
        """
    else:
        system_prompt = """
        Tu es un assistant expert en analyse et synth√®se de rapports d‚Äôhomologation et de suivi
        d‚Äô√©tablissements fran√ßais √† l‚Äô√©tranger.

        Contexte : tu aides l‚Äô√©quipe de pilotage de la Mission La√Øque Fran√ßaise / OSUI (t√™te de r√©seau)
        √† exploiter ces documents. R√©ponds en t‚Äôappuyant uniquement sur les extraits fournis.

        R√®gles de r√©ponse :
        - Appuie-toi uniquement sur les extraits fournis (ne jamais inventer).
        - Structure syst√©matiquement la r√©ponse avec des titres clairs.
        - Mets les √©l√©ments en liste √† puces pour la lisibilit√©.
        - Int√®gre toujours les r√©f√©rences de pages quand elles sont disponibles.
        - Organise la r√©ponse autour des rubriques suivantes si possible :
            * Gouvernance et contexte
            * Atouts et points forts
            * Points de vigilance / critiques
            * Recommandations et axes de travail prioritaires
            * Enjeux pour le pilotage r√©seau (MLF/OSUI)
        - Si la question implique une comparaison avec plusieurs √©tablissements, pr√©sente une
        analyse comparative structur√©e.
        - Termine par une courte synth√®se strat√©gique orient√©e ‚Äút√™te de r√©seau‚Äù.
        """


    user_prompt = f"Question : {query}\n\nExtraits :\n{context}"

    response = client.chat.completions.create(
        model="gpt-5",
        temperature=1,
        messages=[
            {"role": "system", "content": system_prompt.strip()},
            {"role": "user", "content": user_prompt}
        ]
    )

    return response.choices[0].message.content

# ---- UI ----
st.header("üí¨ Smart Chat")

if "messages" not in st.session_state:
    st.session_state["messages"] = []

# Afficher l‚Äôhistorique
for msg in st.session_state["messages"]:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])   # ‚úÖ Markdown rendu correctement

# Input utilisateur
if query := st.chat_input("Pose ta question sur les rapports d‚Äôhomologation..."):
    # Sauvegarde et affichage de la question utilisateur
    st.session_state["messages"].append({"role": "user", "content": query})
    with st.chat_message("user"):
        st.markdown(query)

    # G√©n√©ration et affichage de la r√©ponse
    answer = answer_question(query)
    st.session_state["messages"].append({"role": "assistant", "content": answer})
    with st.chat_message("assistant"):
        st.markdown(answer)   # ‚úÖ Markdown bien rendu
